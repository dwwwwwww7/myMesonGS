#!/usr/bin/env python3
"""
å¿«é€ŸæŸ¥çœ‹ orgb.npz æ–‡ä»¶å†…å®¹å’Œæ•°æ®ç»“æ„çš„è„šæœ¬ï¼Œå‹ç¼©ä¿å­˜çš„æ–¹å¼
é‡ç‚¹æ˜¾ç¤ºæ–‡ä»¶å¤§å°ç»„æˆå’Œæ•°æ®ç»“æ„ï¼Œå¹¶å¯é€šè¿‡--save-txtæŒ‡ä»¤ä¿å­˜ç»ˆç«¯è¾“å‡ºä¸ºTXTæ ¼å¼
ä½¿ç”¨æ¡ˆä¾‹ï¼š
python analyze_orgb_npz.py bins/orgb.npz --save-txt

"""

import sys
import os
import json
import numpy as np
import zipfile
from collections import defaultdict
from datetime import datetime
from io import StringIO

class OutputCapture:
    """æ•è·ç»ˆç«¯è¾“å‡ºçš„ç±»"""
    def __init__(self):
        self.output = StringIO()
        self.original_stdout = sys.stdout
    
    def start_capture(self):
        sys.stdout = self
    
    def stop_capture(self):
        sys.stdout = self.original_stdout
        return self.output.getvalue()
    
    def write(self, text):
        self.original_stdout.write(text)  # åŒæ—¶è¾“å‡ºåˆ°ç»ˆç«¯
        self.output.write(text)  # æ•è·åˆ°å†…å­˜
    
    def flush(self):
        self.original_stdout.flush()

def detect_npz_compression(npz_path):
    """æ£€æµ‹NPZæ–‡ä»¶çš„å‹ç¼©æ ¼å¼"""
    try:
        # NPZæ–‡ä»¶å®é™…ä¸Šæ˜¯ZIPæ–‡ä»¶
        with zipfile.ZipFile(npz_path, 'r') as zf:
            # æ£€æŸ¥ZIPæ–‡ä»¶ä¸­çš„å‹ç¼©æ–¹æ³•
            compression_methods = set()
            for info in zf.infolist():
                compression_methods.add(info.compress_type)
            
            # åˆ¤æ–­å‹ç¼©æ ¼å¼
            if zipfile.ZIP_DEFLATED in compression_methods:
                return "numpy.savez_compressed (ZIP_DEFLATED)"
            elif zipfile.ZIP_STORED in compression_methods:
                return "numpy.savez (ZIP_STORED, æ— å‹ç¼©)"
            else:
                return f"æœªçŸ¥å‹ç¼©æ–¹æ³•: {compression_methods}"
    except:
        return "æ— æ³•æ£€æµ‹å‹ç¼©æ ¼å¼"

def analyze_npz_structure(npz_path, save_txt=False):
    """åˆ†æNPZæ–‡ä»¶çš„æ•°æ®ç»“æ„å’Œå¤§å°ç»„æˆ"""
    
    # åˆ›å»ºè¾“å‡ºæ•è·å™¨
    output_capture = OutputCapture()
    if save_txt:
        output_capture.start_capture()
    
    print("="*80)
    print("orgb.npz æ–‡ä»¶ç»“æ„åˆ†æ")
    print("="*80)
    print(f"æ–‡ä»¶è·¯å¾„: {npz_path}")
    print(f"åˆ†ææ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    
    if not os.path.exists(npz_path):
        print(f"[ERROR] æ–‡ä»¶ä¸å­˜åœ¨")
        if save_txt:
            output_capture.stop_capture()
        return None
    
    total_file_size = os.path.getsize(npz_path)
    print(f"æ–‡ä»¶æ€»å¤§å°: {total_file_size:,} bytes ({total_file_size/1024:.2f} KB, {total_file_size/1024/1024:.2f} MB)")
    
    # æ£€æµ‹å‹ç¼©æ ¼å¼
    compression_format = detect_npz_compression(npz_path)
    print(f"NPZå‹ç¼©æ ¼å¼: {compression_format}")
    
    try:
        data = np.load(npz_path)
        
        print(f"\n" + "="*60)
        print("ã€æ•°æ®ç»“æ„æ¦‚è§ˆã€‘")
        print("="*60)
        print(f"åŒ…å«æ•°ç»„æ•°é‡: {len(data.keys())}")
        print(f"æ•°ç»„é”®å: {sorted(list(data.keys()))}")
        
        # åˆ†æå­˜å‚¨æ ¼å¼
        is_packed = 'packed' in data and len(data['packed']) > 0 and data['packed'][0] == 1
        is_grouped = any(key.startswith('i_') and key.endswith('bit') for key in data.keys())
        is_unified = 'i' in data and not is_grouped and not is_packed
        
        storage_format = "unknown"
        if is_packed:
            storage_format = "bit_packed"
            print(f"  ğŸ“¦ ä½æ‰“åŒ…æ ¼å¼ (Bit-packed format)")
        elif is_grouped:
            storage_format = "grouped"
            print(f"  ğŸ“Š åˆ†ç»„æ ¼å¼ (Grouped format)")
        elif is_unified:
            storage_format = "unified"
            print(f"  ğŸ“„ ç»Ÿä¸€æ ¼å¼ (Unified format)")
        else:
            print(f"  â“ æœªçŸ¥æ ¼å¼")
        

        
        print(f"\n" + "="*60)
        print("ã€æ–‡ä»¶å¤§å°ç»„æˆåˆ†æã€‘")
        print("="*60)
        
        total_uncompressed_size = 0
        arrays_info = []
        
        for key in sorted(data.keys()):
            arr = data[key]
            uncompressed_size = arr.nbytes
            total_uncompressed_size += uncompressed_size
            
            arrays_info.append({
                'key': key,
                'shape': arr.shape,
                'dtype': str(arr.dtype),
                'elements': arr.size,
                'uncompressed_bytes': uncompressed_size,
                'array': arr
            })
        
        # æ˜¾ç¤ºæ¯ä¸ªæ•°ç»„çš„è¯¦ç»†ä¿¡æ¯
        print(f"{'æ•°ç»„å':<15} {'å½¢çŠ¶':<20} {'ç±»å‹':<10} {'å…ƒç´ æ•°':<12} {'æœªå‹ç¼©å¤§å°':<15} {'å æ¯”':<8}")
        print("-" * 90)
        
        for info in arrays_info:
            percentage = (info['uncompressed_bytes'] / total_uncompressed_size) * 100
            size_str = f"{info['uncompressed_bytes']:,} B"
            if info['uncompressed_bytes'] >= 1024:
                size_str += f" ({info['uncompressed_bytes']/1024:.1f} KB)"
            
            print(f"{info['key']:<15} {str(info['shape']):<20} {info['dtype']:<10} {info['elements']:<12,} {size_str:<15} {percentage:>6.1f}%")
        
        print("-" * 90)
        print(f"{'æ€»è®¡':<15} {'':<20} {'':<10} {sum(info['elements'] for info in arrays_info):<12,} {total_uncompressed_size:,} B ({total_uncompressed_size/1024:.1f} KB) {'100.0%':>6}")
        
        # å‹ç¼©æ•ˆç‡
        compression_ratio = total_file_size / total_uncompressed_size
        compression_percentage = (1 - compression_ratio) * 100
        
        print(f"\nå‹ç¼©æ•ˆç‡:")
        print(f"  æœªå‹ç¼©å¤§å°: {total_uncompressed_size:,} bytes ({total_uncompressed_size/1024:.2f} KB)")
        print(f"  å‹ç¼©åå¤§å°: {total_file_size:,} bytes ({total_file_size/1024:.2f} KB)")
        print(f"  å‹ç¼©æ¯”: {compression_ratio:.3f} ({compression_percentage:.1f}% å‡å°‘)")
        
        print(f"\n" + "="*60)
        print("ã€RAHTæ•°æ®ç»“æ„åˆ†æã€‘")
        print("="*60)
        
        # DCç³»æ•°åˆ†æ
        if 'f' in data:
            dc_coeff = data['f']
            print(f"DCç³»æ•° (f):")
            print(f"  å½¢çŠ¶: {dc_coeff.shape}")
            print(f"  å«ä¹‰: RAHTå˜æ¢çš„DCåˆ†é‡ (55ç»´ç‰¹å¾çš„é¢‘åŸŸè¡¨ç¤º)")
            if len(dc_coeff.shape) == 1 and dc_coeff.shape[0] == 55:
                print(f"  ç»“æ„: opacity(1) + euler(3) + f_dc(3) + f_rest(45) + scale(3)")
                print(f"  æ•°å€¼èŒƒå›´: [{dc_coeff.min():.4f}, {dc_coeff.max():.4f}]")
                
                # æ˜¾ç¤º55ç»´ç‰¹å¾çš„åˆ†ç»„
                feature_groups = [
                    ("opacity", 0, 1, "é€æ˜åº¦"),
                    ("euler", 1, 4, "æ¬§æ‹‰è§’(æ—‹è½¬)"),
                    ("f_dc", 4, 7, "SHç³»æ•°0é˜¶(åŸºç¡€é¢œè‰²)"),
                    ("f_rest", 7, 52, "SHç³»æ•°1-3é˜¶(é¢œè‰²ç»†èŠ‚)"),
                    ("scale", 52, 55, "ç¼©æ”¾å‚æ•°")
                ]
                
                print(f"  è¯¦ç»†ç»“æ„:")
                for name, start, end, desc in feature_groups:
                    values = dc_coeff[start:end]
                    print(f"    {name:8s} [{start:2d}:{end:2d}]: {desc:15s} èŒƒå›´[{values.min():8.4f}, {values.max():8.4f}]")
        
        # ACç³»æ•°åˆ†æ
        print(f"\nACç³»æ•° (é¢‘åŸŸç»†èŠ‚):")
        
        if is_grouped:
            print(f"  å­˜å‚¨æ–¹å¼: æŒ‰é‡åŒ–ä½æ•°åˆ†ç»„å­˜å‚¨")
            
            # ç»Ÿè®¡å„ä¸ªä½å®½ç»„
            bit_groups = defaultdict(dict)
            total_ac_size = 0
            
            for key in data.keys():
                if key.startswith('i_') and key.endswith('bit'):
                    bit = int(key.split('_')[1].replace('bit', ''))
                    group_data = data[key]
                    dims_key = f'dims_{bit}bit'
                    
                    if dims_key in data:
                        dims = data[dims_key]
                        bit_groups[bit] = {
                            'data': group_data,
                            'dims': dims,
                            'shape': group_data.shape,
                            'size_bytes': group_data.nbytes,
                            'nonzero_ratio': np.count_nonzero(group_data) / group_data.size if group_data.size > 0 else 0
                        }
                        total_ac_size += group_data.nbytes
            
            print(f"  åˆ†ç»„è¯¦æƒ…:")
            print(f"    {'ä½å®½':<6} {'ç»´åº¦æ•°':<8} {'æ•°æ®å½¢çŠ¶':<20} {'å¤§å°':<15} {'ç¨€ç–åº¦':<10} {'ç»´åº¦ç´¢å¼•ç¤ºä¾‹'}")
            print(f"    {'-'*6} {'-'*8} {'-'*20} {'-'*15} {'-'*10} {'-'*20}")
            
            for bit in sorted(bit_groups.keys()):
                info = bit_groups[bit]
                sparsity = (1 - info['nonzero_ratio']) * 100
                size_str = f"{info['size_bytes']:,} B"
                if info['size_bytes'] >= 1024:
                    size_str += f" ({info['size_bytes']/1024:.1f}KB)"
                
                dims_preview = str(info['dims'][:5].tolist()) if len(info['dims']) > 5 else str(info['dims'].tolist())
                if len(info['dims']) > 5:
                    dims_preview = dims_preview[:-1] + ",...]"
                
                print(f"    {bit:>4d}   {len(info['dims']):>6d}   {str(info['shape']):<20} {size_str:<15} {sparsity:>6.1f}%   {dims_preview}")
            
            print(f"  ACç³»æ•°æ€»å¤§å°: {total_ac_size:,} bytes ({total_ac_size/1024:.2f} KB)")
            
            # åˆ†æç»´åº¦åˆ†é…
            print(f"\n  ç»´åº¦åˆ†é…ç­–ç•¥:")
            all_dims_used = set()
            for bit in sorted(bit_groups.keys()):
                dims = bit_groups[bit]['dims']
                all_dims_used.update(dims)
                
                # æ¨æ–­ç»´åº¦å«ä¹‰
                dim_meanings = []
                for dim in dims[:3]:  # åªæ˜¾ç¤ºå‰3ä¸ªç»´åº¦çš„å«ä¹‰
                    if dim == 0:
                        dim_meanings.append("opacity")
                    elif 1 <= dim <= 3:
                        dim_meanings.append(f"euler_{dim-1}")
                    elif 4 <= dim <= 6:
                        dim_meanings.append(f"f_dc_{dim-4}")
                    elif 7 <= dim <= 51:
                        dim_meanings.append(f"f_rest_{dim-7}")
                    elif 52 <= dim <= 54:
                        dim_meanings.append(f"scale_{dim-52}")
                    else:
                        dim_meanings.append(f"unknown_{dim}")
                
                meanings_str = ", ".join(dim_meanings)
                if len(dims) > 3:
                    meanings_str += ", ..."
                
                print(f"    {bit:2d}-bit: {meanings_str}")
            
            print(f"  æ€»ç»´åº¦è¦†ç›–: {len(all_dims_used)}/55 ({'å®Œæ•´' if len(all_dims_used) == 55 else 'ä¸å®Œæ•´'})")
        
        elif is_unified:
            ac_data = data['i']
            print(f"  å­˜å‚¨æ–¹å¼: ç»Ÿä¸€å­˜å‚¨")
            print(f"  æ•°æ®å½¢çŠ¶: {ac_data.shape}")
            print(f"  æ•°æ®å¤§å°: {ac_data.nbytes:,} bytes ({ac_data.nbytes/1024:.2f} KB)")
            
            if len(ac_data.shape) == 1:
                total_elements = ac_data.shape[0]
                if total_elements % 55 == 0:
                    n_points = total_elements // 55
                    print(f"  æ¨æ–­ç»“æ„: {n_points:,} ä¸ªACç‚¹ Ã— 55 ç»´ç‰¹å¾")
                else:
                    print(f"  æ³¨æ„: æ€»å…ƒç´ æ•° {total_elements:,} ä¸èƒ½è¢«55æ•´é™¤")
            
            nonzero_ratio = np.count_nonzero(ac_data) / ac_data.size if ac_data.size > 0 else 0
            sparsity = (1 - nonzero_ratio) * 100
            print(f"  ç¨€ç–åº¦: {sparsity:.2f}% (éé›¶å…ƒç´ : {nonzero_ratio*100:.2f}%)")
        
        elif is_packed:
            print(f"  å­˜å‚¨æ–¹å¼: ä½çº§æ‰“åŒ…å­˜å‚¨")
            if 'i' in data:
                bitstream = data['i']
                print(f"  ä½æµå¤§å°: {len(bitstream):,} bytes")
                print(f"  æ€»ä½æ•°: {len(bitstream) * 8:,} bits")
        
        # å…¶ä»–è¾…åŠ©æ•°æ®
        print(f"\nè¾…åŠ©æ•°æ®:")
        for key in sorted(data.keys()):
            if key not in ['f', 'i'] and not (key.startswith('i_') and key.endswith('bit')) and not key.startswith('dims_'):
                arr = data[key]
                print(f"  {key}: {arr.shape} {arr.dtype} ({arr.nbytes} bytes)")
                if arr.size <= 10:
                    print(f"    æ•°æ®: {arr}")
        
        data.close()
        
        # ä¿å­˜TXTæ–‡ä»¶
        if save_txt:
            captured_output = output_capture.stop_capture()
            txt_path = npz_path.replace('.npz', '_analysis.txt')
            try:
                with open(txt_path, 'w', encoding='utf-8') as f:
                    f.write(captured_output)
                print(f"\nâœ“ åˆ†æç»“æœå·²ä¿å­˜åˆ°: {txt_path}")
            except Exception as e:
                print(f"\nâœ— ä¿å­˜TXTæ–‡ä»¶å¤±è´¥: {e}")
        
        print(f"\n" + "="*80)
        print("åˆ†æå®Œæˆ")
        print("="*80)
        
    except Exception as e:
        print(f"[ERROR] åˆ†æå¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        
        if save_txt:
            output_capture.stop_capture()

def main():
    if len(sys.argv) < 2:
        print("ç”¨æ³•: python analyze_orgb_npz.py <orgb.npzè·¯å¾„> [--save-txt]")
        print()
        print("å‚æ•°:")
        print("  orgb.npzè·¯å¾„    è¦åˆ†æçš„NPZæ–‡ä»¶è·¯å¾„")
        print("  --save-txt      ä¿å­˜ç»ˆç«¯è¾“å‡ºä¸ºTXTæ ¼å¼")
        print()
        print("ç¤ºä¾‹:")
        print("  python analyze_orgb_npz.py bins/orgb.npz")
        print("  python analyze_orgb_npz.py bins/orgb.npz --save-txt")
        print("  python analyze_orgb_npz.py \"E:/path/to/orgb.npz\" --save-txt")
        sys.exit(1)
    
    npz_path = sys.argv[1]
    save_txt = '--save-txt' in sys.argv
    
    analyze_npz_structure(npz_path, save_txt=save_txt)

if __name__ == "__main__":
    main()



def analyze_first_n_gaussians(npz_path, n=100):
    """
    åˆ†æå‰Nä¸ªé«˜æ–¯ç‚¹çš„è¯¦ç»†å±æ€§å€¼
    
    Args:
        npz_path: NPZæ–‡ä»¶è·¯å¾„
        n: è¦åˆ†æçš„é«˜æ–¯ç‚¹æ•°é‡ï¼ˆé»˜è®¤100ï¼‰
    """
    print("\n" + "="*70)
    print(f"ã€å‰{n}ä¸ªé«˜æ–¯ç‚¹è¯¦ç»†åˆ†æã€‘")
    print("="*70)
    
    data = np.load(npz_path)
    
    # è·å–DCç³»æ•°
    dc_coeff = data['f']
    print(f"\nDCç³»æ•° (1ä¸ªç‚¹):")
    print(f"  å½¢çŠ¶: {dc_coeff.shape}")
    print(f"  å€¼: {dc_coeff}")
    
    # æ£€æµ‹å­˜å‚¨æ ¼å¼
    is_packed = 'packed' in data and data['packed'][0] == 1
    is_grouped = 'grouped' in data and data['grouped'][0] == 1
    
    # è®¡ç®—é«˜æ–¯ç‚¹æ•°é‡
    if 'i' in data:
        if is_packed and 'bit_config' in data:
            bit_config = data['bit_config'].tolist()
            total_bits_per_point = sum(bit_config)
            bitstream = bytes(data["i"])
            total_bits = len(bitstream) * 8
            n_points = total_bits // total_bits_per_point
            print(f"\né«˜æ–¯ç‚¹æ•°é‡: {n_points:,}")
            print(f"  (ä»ä½æµå¤§å°æ¨ç®—: {len(bitstream):,} bytes Ã— 8 / {total_bits_per_point} bits/point)")
        else:
            ac_data = data['i']
            if len(ac_data.shape) == 2:
                n_points = ac_data.shape[0]
                print(f"\né«˜æ–¯ç‚¹æ•°é‡: {n_points:,}")
            elif len(ac_data.shape) == 1 and ac_data.shape[0] % 55 == 0:
                n_points = ac_data.shape[0] // 55
                print(f"\né«˜æ–¯ç‚¹æ•°é‡: {n_points:,}")
                print(f"  (ä»æ•°ç»„å¤§å°æ¨ç®—: {ac_data.shape[0]:,} / 55)")
    
    if is_packed:
        print(f"\næ£€æµ‹åˆ°ä½æ‰“åŒ…æ ¼å¼")
        bitstream = bytes(data["i"])
        bit_config = data['bit_config'].tolist()
        
        print(f"  ä½å®½é…ç½®: {bit_config}")
        print(f"  æ€»ä½æ•°/ç‚¹: {sum(bit_config)} bits")
        
        # è§£åŒ…å‰Nä¸ªé«˜æ–¯ç‚¹
        print(f"\næ­£åœ¨è§£åŒ…å‰{min(n, n_points)}ä¸ªé«˜æ–¯ç‚¹...")
        actual_n = min(n, n_points)
        first_n_data = np.zeros((actual_n, 55), dtype=np.float32)
        
        bit_pos = 0
        for i in range(actual_n):
            for c in range(55):
                bits = bit_config[c]
                value = 0
                
                # è¯»å–ä½æµ
                for b in range(bits):
                    byte_idx = bit_pos // 8
                    bit_idx = bit_pos % 8
                    if byte_idx < len(bitstream) and bitstream[byte_idx] & (1 << bit_idx):
                        value |= (1 << b)
                    bit_pos += 1
                
                first_n_data[i, c] = value
        
    elif is_grouped:
        print(f"\næ£€æµ‹åˆ°åˆ†ç»„å­˜å‚¨æ ¼å¼")
        print(f"  (æš‚ä¸æ”¯æŒè¯¦ç»†è§£æåˆ†ç»„æ ¼å¼)")
        return
    else:
        print(f"\næœªçŸ¥æ ¼å¼æˆ–ç»Ÿä¸€å­˜å‚¨æ ¼å¼")
        if 'i' in data:
            ac_data = data['i']
            if len(ac_data.shape) == 2:
                actual_n = min(n, ac_data.shape[0])
                first_n_data = ac_data[:actual_n]
                print(f"  æå–å‰{actual_n}ä¸ªç‚¹")
            else:
                print(f"  æ— æ³•è§£ææ•°æ®æ ¼å¼")
                return
        else:
            return
    
    # æ˜¾ç¤ºå‰Nä¸ªé«˜æ–¯ç‚¹çš„ç»Ÿè®¡
    print(f"\nå‰{actual_n}ä¸ªé«˜æ–¯ç‚¹çš„å±æ€§ç»Ÿè®¡:")
    print("-"*70)
    
    feature_groups = [
        ("opacity", 0, 1, "é€æ˜åº¦"),
        ("euler", 1, 4, "æ¬§æ‹‰è§’(æ—‹è½¬)"),
        ("f_dc", 4, 7, "SHç³»æ•°0é˜¶(åŸºç¡€é¢œè‰²)"),
        ("f_rest_0", 7, 22, "SHç³»æ•°1é˜¶(15ç»´)"),
        ("f_rest_1", 22, 37, "SHç³»æ•°2é˜¶(15ç»´)"),
        ("f_rest_2", 37, 52, "SHç³»æ•°3é˜¶(15ç»´)"),
        ("scale", 52, 55, "ç¼©æ”¾å‚æ•°"),
    ]
    
    for name, start, end, desc in feature_groups:
        values = first_n_data[:, start:end]
        print(f"\n{name:12s} [{start:2d}:{end:2d}]: {desc}")
        print(f"  å½¢çŠ¶: {values.shape}")
        print(f"  èŒƒå›´: [{values.min():.4f}, {values.max():.4f}]")
        print(f"  å‡å€¼: {values.mean():.4f}")
        print(f"  æ ‡å‡†å·®: {values.std():.4f}")
        
        # æ˜¾ç¤ºå‰5ä¸ªç‚¹çš„å€¼
        if end - start <= 3:
            print(f"  å‰5ä¸ªç‚¹:")
            for i in range(min(5, actual_n)):
                vals = values[i]
                vals_str = " ".join([f"{v:8.2f}" for v in vals])
                print(f"    ç‚¹{i}: {vals_str}")
    
    # ä¿å­˜è¯¦ç»†æ•°æ®åˆ°æ–‡ä»¶
    output_dir = os.path.dirname(npz_path)
    if not output_dir:
        output_dir = '.'
    output_file = os.path.join(output_dir, f'first_{actual_n}_gaussians.txt')
    
    with open(output_file, 'w', encoding='utf-8') as f:
        f.write(f"å‰{actual_n}ä¸ªé«˜æ–¯ç‚¹çš„è¯¦ç»†å±æ€§å€¼\n")
        f.write(f"æ–‡ä»¶: {npz_path}\n")
        f.write("="*70 + "\n\n")
        
        f.write("DCç³»æ•°:\n")
        f.write(f"{dc_coeff}\n\n")
        
        f.write(f"ACç³»æ•° (å‰{actual_n}ä¸ªé«˜æ–¯ç‚¹):\n")
        f.write("-"*70 + "\n")
        
        # å†™å…¥è¡¨å¤´
        f.write(f"{'ç‚¹ID':>6s} | ")
        for name, start, end, _ in feature_groups:
            for i in range(start, end):
                f.write(f"{name}_{i-start:02d} ")
        f.write("\n")
        f.write("-"*200 + "\n")
        
        # å†™å…¥æ•°æ®
        for i in range(actual_n):
            f.write(f"{i:6d} | ")
            for j in range(55):
                f.write(f"{first_n_data[i, j]:8.2f} ")
            f.write("\n")
    
    print(f"\nâœ“ è¯¦ç»†æ•°æ®å·²ä¿å­˜åˆ°: {output_file}")
    data.close()


def compare_two_npz(npz_path1, npz_path2, n=100):
    """
    å¯¹æ¯”ä¸¤ä¸ªNPZæ–‡ä»¶çš„å·®å¼‚
    
    Args:
        npz_path1: ç¬¬ä¸€ä¸ªNPZæ–‡ä»¶è·¯å¾„
        npz_path2: ç¬¬äºŒä¸ªNPZæ–‡ä»¶è·¯å¾„
        n: å¯¹æ¯”å‰Nä¸ªé«˜æ–¯ç‚¹
    """
    print("\n" + "="*70)
    print(f"ã€å¯¹æ¯”ä¸¤ä¸ªNPZæ–‡ä»¶ã€‘")
    print("="*70)
    print(f"æ–‡ä»¶1: {npz_path1}")
    print(f"æ–‡ä»¶2: {npz_path2}")
    
    data1 = np.load(npz_path1)
    data2 = np.load(npz_path2)
    
    # å¯¹æ¯”DCç³»æ•°
    dc1 = data1['f']
    dc2 = data2['f']
    
    print(f"\nDCç³»æ•°å¯¹æ¯”:")
    print(f"  æ–‡ä»¶1: min={dc1.min():.6f}, max={dc1.max():.6f}, mean={dc1.mean():.6f}")
    print(f"  æ–‡ä»¶2: min={dc2.min():.6f}, max={dc2.max():.6f}, mean={dc2.mean():.6f}")
    
    dc_diff = dc1 - dc2
    print(f"  å·®å¼‚: min={dc_diff.min():.6f}, max={dc_diff.max():.6f}")
    print(f"  æœ€å¤§ç»å¯¹å·®å¼‚: {np.abs(dc_diff).max():.6f}")
    print(f"  å¹³å‡ç»å¯¹å·®å¼‚: {np.abs(dc_diff).mean():.6f}")
    
    # æ˜¾ç¤ºå·®å¼‚æœ€å¤§çš„ç»´åº¦
    max_diff_idx = np.abs(dc_diff).argmax()
    print(f"  å·®å¼‚æœ€å¤§çš„ç»´åº¦: {max_diff_idx} (å·®å¼‚={dc_diff[max_diff_idx]:.6f})")
    
    # å¯¹æ¯”ACç³»æ•°
    if 'i' in data1 and 'i' in data2:
        size1 = len(data1['i'])
        size2 = len(data2['i'])
        print(f"\nACç³»æ•°å¤§å°å¯¹æ¯”:")
        print(f"  æ–‡ä»¶1: {size1:,} bytes")
        print(f"  æ–‡ä»¶2: {size2:,} bytes")
        if size1 != size2:
            print(f"  å·®å¼‚: {size2 - size1:,} bytes ({(size2-size1)/size1*100:.2f}%)")
        else:
            print(f"  å¤§å°ç›¸åŒ")
            
            # å¦‚æœå¤§å°ç›¸åŒï¼Œå¯¹æ¯”å†…å®¹
            ac1 = data1['i']
            ac2 = data2['i']
            if np.array_equal(ac1, ac2):
                print(f"  å†…å®¹å®Œå…¨ç›¸åŒ")
            else:
                diff_count = np.sum(ac1 != ac2)
                print(f"  å†…å®¹ä¸åŒ: {diff_count:,} / {len(ac1):,} å­—èŠ‚ä¸åŒ ({diff_count/len(ac1)*100:.2f}%)")
    
    # å¯¹æ¯”æ–‡ä»¶å¤§å°
    import os
    file1_size = os.path.getsize(npz_path1)
    file2_size = os.path.getsize(npz_path2)
    print(f"\næ–‡ä»¶å¤§å°å¯¹æ¯”:")
    print(f"  æ–‡ä»¶1: {file1_size:,} bytes ({file1_size/1024:.2f} KB)")
    print(f"  æ–‡ä»¶2: {file2_size:,} bytes ({file2_size/1024:.2f} KB)")
    if file1_size != file2_size:
        print(f"  å·®å¼‚: {file2_size - file1_size:,} bytes ({(file2_size-file1_size)/file1_size*100:.2f}%)")
    else:
        print(f"  å¤§å°ç›¸åŒ")
    
    data1.close()
    data2.close()


def main_extended():
    """æ‰©å±•çš„ä¸»å‡½æ•°ï¼Œæ”¯æŒæ›´å¤šåŠŸèƒ½"""
    import sys
    
    if len(sys.argv) < 2:
        print("ç”¨æ³•:")
        print("  1. åŸºç¡€åˆ†æ:     python analyze_orgb_npz.py <npzæ–‡ä»¶è·¯å¾„> [--save-txt]")
        print("  2. è¯¦ç»†åˆ†æ:     python analyze_orgb_npz.py <npzæ–‡ä»¶è·¯å¾„> --detail [é«˜æ–¯ç‚¹æ•°é‡]")
        print("  3. å¯¹æ¯”ä¸¤ä¸ªæ–‡ä»¶: python analyze_orgb_npz.py <æ–‡ä»¶1> <æ–‡ä»¶2> --compare [é«˜æ–¯ç‚¹æ•°é‡]")
        print("\nç¤ºä¾‹:")
        print("  python analyze_orgb_npz.py output/truck/bins/orgb.npz")
        print("  python analyze_orgb_npz.py output/truck/bins/orgb.npz --detail 200")
        print("  python analyze_orgb_npz.py file1.npz file2.npz --compare 100")
        sys.exit(1)
    
    npz_path = sys.argv[1]
    
    # æ£€æŸ¥æ˜¯å¦æ˜¯å¯¹æ¯”æ¨¡å¼
    if '--compare' in sys.argv and len(sys.argv) >= 3:
        npz_path2 = sys.argv[2]
        n = 100
        if len(sys.argv) >= 5:
            try:
                n = int(sys.argv[4])
            except:
                pass
        
        print("="*70)
        print("æ–‡ä»¶1åˆ†æ:")
        print("="*70)
        analyze_npz_structure(npz_path, save_txt=False)
        
        print("\n" + "="*70)
        print("æ–‡ä»¶2åˆ†æ:")
        print("="*70)
        analyze_npz_structure(npz_path2, save_txt=False)
        
        compare_two_npz(npz_path, npz_path2, n)
        
    elif '--detail' in sys.argv:
        # è¯¦ç»†åˆ†ææ¨¡å¼
        n = 100
        if len(sys.argv) >= 4:
            try:
                n = int(sys.argv[3])
            except:
                pass
        
        analyze_npz_structure(npz_path, save_txt=False)
        analyze_first_n_gaussians(npz_path, n)
    else:
        # åŸºç¡€åˆ†ææ¨¡å¼
        save_txt = '--save-txt' in sys.argv
        analyze_npz_structure(npz_path, save_txt=save_txt)


# å¦‚æœç›´æ¥è¿è¡Œæ­¤è„šæœ¬ï¼Œä½¿ç”¨æ‰©å±•çš„ä¸»å‡½æ•°
if __name__ == "__main__" and len(sys.argv) > 1 and ('--detail' in sys.argv or '--compare' in sys.argv):
    main_extended()
